# CVE_Jacksonville


1) **Area Segmentation:** The area segmentation portion of the code asks for user input in selecting the desired areas in an image and outputs a mask. For each area, the user clicks where they want each corner of the area to be, right clicking for the final point, and the code adds the area onto the image in red lines as the user clicks. When the user is done, the code asks for the name of the area, and saves the area onto a mask, which is all black, except for the pixels within the area, which are assigned a color from a list. The code saves the mask and a json file with a dictionary that contains key value pairs of the area name and its assigned color. This information is used in the real time analysis of video. This code is integrated with the ui in ui.py



2) **Status Detection:** - The concept of status detection in the code involves visually representing the status or position of detected individuals within the video stream. For each identified person (labeled as 'person' by YOLO), the script draws bounding boxes encompassing the top-left, top-right, bottom-left, and bottom-right corners, as well as a circle at the bottom midpoint. These visual indicators convey the spatial orientation of a person in the frame. Additionally, the script defines specific areas of interest (For eg. Hallway, Bathroom, Table or Waiting Area, Queue, Sitting Area) using polylines, assigning labels to each area. The combination of YOLO and status detection provides a comprehensive understanding of object positions and their visual representation in real-time video analysis.


3) **YOLO:** - YOLO, or You Only Look Once, is a real-time object detection algorithm employed in the provided code to identify and locate objects within a video stream. YOLO differs from traditional detection methods by dividing the input image into a grid and making predictions for multiple objects in a single pass through the neural network. This allows YOLO to achieve remarkable speed in object detection. In the context of the script, YOLO processes video frames, detecting people labeled as 'person' and providing bounding box coordinates along with class probabilities. The algorithm's efficiency makes it suitable for applications requiring quick and accurate object recognition.


4) **Live Occupancy:** The Live Occupancy portion of the provided code focuses on real-time object detection using the YOLO (You Only Look Once) model. Utilizing the 'yolov8s.pt' file, the YOLO model processes frames captured from a video source, identifying individuals classified as "person" within the scene. The code visualizes these detections by drawing bounding boxes around detected people, labeling them as "person," and marking the key bottom-midpoint with circle. Additionally, the code defines and displays different areas that are drawn by the user using polygons and labels. The user is provided with a real-time view of detected individuals and their spatial distribution within the specified regions. The program allows for easy observation of live occupancy dynamics and is terminated upon user input (pressing the 'Esc' key), ensuring an interactive and informative experience.


5) **UI:** The purpose of creating code for a user interface (UI) to demonstrate a proof of concept is to visually articulate the intended design and functionality of a software application. By implementing UI code, we can show a clear proof of concept. This proof of concept helps validate the feasibility and potential success of the proposed product. Running this part of the code requires instructions.png, analytics.png, first_sidebar.png, and first_frame.png to be in the same folder. The workflow of the code is as follows: the user will click anywhere on instructions.png to continue, select their areas on the left side (where their area is), double click in the area that prompts the user to advance, and then watch the live analytics on the last slide.
